{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c93b505b",
   "metadata": {},
   "source": [
    "# TOPICS [5TH JUNE 2025]:\n",
    "\n",
    "- Epochs.\n",
    "- Batch Size.\n",
    "- Iteration.\n",
    "- Learning Rate.\n",
    "- Parameters vs Hyperparameters\n",
    "\n",
    "---\n",
    "\n",
    "## EPOCHS:\n",
    "\n",
    "- When an entire training set is passed through algorithm [both forward & backward -> for NNs] then it is said to be a 1 epoch.\n",
    "\n",
    "- e.g.: \n",
    "Training samples length -> 1000 [rows / training data samples]\n",
    "if EPOCHS = 20 -> Then algorithm will be processing each one of the sample from that Training samples 20 times...\n",
    "\n",
    "## BATCH & BATCH SIZE: \n",
    "\n",
    "- The training set/data is divided into small batches (chunks) to avoid the storage space limitations and other limitations of a computer system.\n",
    "- *HYPERPARAMETER Batch Size* is basically the number of samples per batch/chunk.\n",
    "- This refers to the subset of the training data that is processed together (**1 forward pass & 1 backward pass** of NN).\n",
    "\n",
    "e.g.:\n",
    "- Training samples length -> 1000 samples\n",
    "    - If EPOCH = 5: all 1000 samples will be processed 5 times.\n",
    "    - There will be total 5 iterations here...\n",
    "\n",
    "- let BATCH_SIZE = 500:\n",
    "    - Here 500 samples will be processed together and then parameters are updated (i.e. weights & bias and all)\n",
    "    - batch_1 : first 500 samples [processing] -> iteration 1\n",
    "    - batch_2 : second 500 samples [processing] -> iteration 2\n",
    "\n",
    "    - This batch_1 and batch_2 are processed at every epochs. That means there are total 2 iterations in 1 epoch.\n",
    "\n",
    "## Iterations:\n",
    "\n",
    "- As seen in above example, an iteration is counted when a single batch is processed.\n",
    "- At every iteration the parameters are updated [Ws & Bs]\n",
    "- In NN at every iteration following things happen:\n",
    "    - **Forward Propagation:** Input data passed through n/w to generate the predictions & loss is calculated [here we use activation functions to find non-linearity].\n",
    "    - **Backward Propagation:** Calculated loss are used to calculate the gradient & parameters are updated to reduce the loss [we here use optimizers]\n",
    "\n",
    "```\n",
    "No. of iterations (per epoch) = len(X_train) / batch_size\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7b9db0",
   "metadata": {},
   "source": [
    "## Learning Rate:\n",
    "\n",
    "- In any model (especially deep learning one), during the training process. Model's parameters are updated frequently in order to minimize the loss. So learning rate basically controls how much to change the model's parameters wrt loss calculated. *(we have already seen this during study of optimizers)*\n",
    "\n",
    "- lr is basically the step size the model takes towards minimizing the loss.\n",
    "\n",
    "- LOW LR: model updates its parameters too slowly and take too long to reach convergence.\n",
    "- HIGH LR: model updates its parameters too frequently and leads to overshooting.\n",
    "- Optimal LR: not too low not too high lr.\n",
    "\n",
    "Best Practices to determine the Learning Rate: \n",
    "- Starting with low lr, and gradually increasing it in multicaptive way [0.001, 0.01, 0.1] - along with monitoring the loss.\n",
    "- Using *Optimizers* that automatically adjust the learning rate like **Adam** or **RMSProp**\n",
    "\n",
    "```\n",
    "w(i) = w(i-1) + lr * g(loss) \n",
    "\n",
    "w -> weights\n",
    "lr -> learning rate\n",
    "g(loss) -> gradient of loss function [partial derivative - to find the slope]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48e8fe8",
   "metadata": {},
   "source": [
    "## Parameters & Hyperparameters:\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- These are internal variables of the model learned/tuned during model training from the training data.\n",
    "- Paremeters are initialized randomly initially and updated via gradient descent during training. \n",
    "- Stored in model checkpoints when saved (e.g.: SafeTensors file for LLMs consists of trained params)\n",
    "\n",
    "e.g.:\n",
    "- In Linear Regression: thetas are parameter (slopes and intercept)\n",
    "- In NN: Weights & bias (at each layer) are parameters\n",
    "- In LLM (Transformer based networks: BERT and all): Attention weights, feed forward layer weights, embedding matrices\n",
    "\n",
    "\n",
    "### Hyperparameters:\n",
    "\n",
    "- Hyperparameters are externel settings determined by us or auto-tuners which are set before training process start. So hyperparameters are not learned during the training.\n",
    "\n",
    "- Setting up right parameters for the model leads to the best results, it can be done manuallly and there are various methods to perform the hyperparam. tuning like **Grid Search CV, Bayesian Optimization, Random Search**, etc\n",
    "\n",
    "\n",
    "e.g. of hyperparams:\n",
    "- Epochs\n",
    "- Batch Size\n",
    "- Learning rate\n",
    "- Dropout rate\n",
    "- No. of layers (in dl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37727ad1",
   "metadata": {},
   "source": [
    "---\n",
    "By Kirtan Ghelani @SculptSoft"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
